{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Balancing RNN sequence data-predicting RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOyQiLUiGVh7HinOj/5vhkT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Papadopoulos18/Cryptocurrency-predicting-RNN-BTC-LTC-BCH-ETH-with-Tensorflow/blob/main/Balancing_RNN_sequence_data_predicting_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Hj0bBW6fMH"
      },
      "source": [
        "# 1st structuring our data \r\n",
        "1. we got the data \r\n",
        "2. we merge the data\r\n",
        "3. we create targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v42a_aYr2uOn"
      },
      "source": [
        "import pandas as pd \r\n",
        "import os\r\n",
        "\r\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d15RfNC6-Yz1"
      },
      "source": [
        "upload the 4 .csv files manualy here on Google Colab as path:\"/content/.csv\" (link to download the data from:https://pythonprogramming.net/static/downloads/machine-learning-data/crypto_data.zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUvQHO_U_DHw"
      },
      "source": [
        "we are going to name the columns of the .csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htdNz1Lg-LvI",
        "outputId": "c8ffd8bb-93fb-4e60-99e7-1293f3de95ba"
      },
      "source": [
        "df = pd.read_csv(\"/content/LTC-USD.csv\", names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\r\n",
        "print(df.head())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         time        low       high       open      close      volume\n",
            "0  1528968660  96.580002  96.589996  96.589996  96.580002    9.647200\n",
            "1  1528968720  96.449997  96.669998  96.589996  96.660004  314.387024\n",
            "2  1528968780  96.470001  96.570000  96.570000  96.570000   77.129799\n",
            "3  1528968840  96.449997  96.570000  96.570000  96.500000    7.216067\n",
            "4  1528968900  96.279999  96.540001  96.500000  96.389999  524.539978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJN1Qu1n_27b"
      },
      "source": [
        "we want to get the close and the volume for each one of the 4 .csv files. The only thing that these 4 csv files have in common is the \"time\" column. They all share the same index, which is time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovqYw0oK-8DW",
        "outputId": "0587dc27-0094-43cf-a7bd-f8c27cefefb8"
      },
      "source": [
        "main_df = pd.DataFrame() # begin empty\r\n",
        "\r\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\r\n",
        "for ratio in ratios:\r\n",
        "  print(ratio)\r\n",
        "  dataset = f\"/content/{ratio}.csv\"\r\n",
        "  # print(dataset)\r\n",
        "\r\n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\r\n",
        "  # print(df.head()) we want to work with close and volume\r\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\r\n",
        "\r\n",
        "  df.set_index(\"time\", inplace=True)\r\n",
        "  df = df[[f'{ratio}_close',f\"{ratio}_volume\"]] # ignore the other columns besides price and volume\r\n",
        "  # print(df.head())\r\n",
        "\r\n",
        "  # now we want to merge those 4\r\n",
        "  if len(main_df) == 0:           #i.e. is empty\r\n",
        "    main_df = df\r\n",
        "  else:\r\n",
        "    main_df = main_df.join(df)\r\n",
        "\r\n",
        "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\r\n",
        "main_df.dropna(inplace=True)\r\n",
        "print(main_df.head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BTC-USD\n",
            "LTC-USD\n",
            "BCH-USD\n",
            "ETH-USD\n",
            "            BTC-USD_close  BTC-USD_volume  ...  ETH-USD_close  ETH-USD_volume\n",
            "time                                       ...                               \n",
            "1528968720    6487.379883        7.706374  ...      486.01001       26.019083\n",
            "1528968780    6479.410156        3.088252  ...      486.00000        8.449400\n",
            "1528968840    6479.410156        1.404100  ...      485.75000       26.994646\n",
            "1528968900    6479.979980        0.753000  ...      486.00000       77.355759\n",
            "1528968960    6480.000000        1.490900  ...      486.00000        7.503300\n",
            "\n",
            "[5 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2SUslc8H_OV"
      },
      "source": [
        "Next, we need to create a target. To do this, we need to know which price we're trying to predict. We also need to know how far out we want to predict. We'll go with Litecoin for now. Knowing how far out we want to predict probably also depends how long our sequences are. If our sequence length is 3 (so...3 minutes), we probably can't easily predict out 10 minutes. If our sequence length is 300, 10 might not be as hard. I'd like to go with a sequence length of 60, and a future prediction out of 3. We could also make the prediction a regression question, using a linear activation with the output layer, but, instead, I am going to just go with a binary classification.\r\n",
        "\r\n",
        "If price goes up in 3 minutes, then it's a buy. If it goes down in 3 minutes, not buy/sell. With all of that in mind, I am going to make the following constants:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9qDv3JcDLLS"
      },
      "source": [
        "SEQ_LEN = 60\r\n",
        "FUTURE_PERIOD_PREDICT = 3\r\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\r\n",
        "\r\n",
        "def classify(current, future):\r\n",
        "  if float(futute)>float(current):\r\n",
        "    return 1                        #BUY\r\n",
        "  else:\r\n",
        "    return 0                        #DONT BUY"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phK9S5-SLW77"
      },
      "source": [
        "## so knowing these we are writing our code(1st part) like below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj_GuoQEKvsP",
        "outputId": "49c9cd0c-e114-4cbd-ffb3-2a569ba6aa91"
      },
      "source": [
        "import pandas as pd \r\n",
        "import os\r\n",
        "\r\n",
        "SEQ_LEN = 60\r\n",
        "FUTURE_PERIOD_PREDICT = 3\r\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\r\n",
        "\r\n",
        "\r\n",
        "def classify(current, future):\r\n",
        "  if float(future)>float(current):\r\n",
        "    return 1                        #BUY\r\n",
        "  else:\r\n",
        "    return 0                        #DONT BUY\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "main_df = pd.DataFrame() # begin empty\r\n",
        "\r\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\r\n",
        "for ratio in ratios:\r\n",
        "  # print(ratio)\r\n",
        "  dataset = f\"/content/{ratio}.csv\"\r\n",
        "  # print(dataset)\r\n",
        "\r\n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\r\n",
        "  # print(df.head()) we want to work with close and volume\r\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\r\n",
        "\r\n",
        "  df.set_index(\"time\", inplace=True)\r\n",
        "  df = df[[f'{ratio}_close',f\"{ratio}_volume\"]]\r\n",
        "  # print(df.head())\r\n",
        "\r\n",
        "  # now we want to merge those 4\r\n",
        "  if len(main_df) == 0:           #i.e. is empty\r\n",
        "    main_df = df\r\n",
        "  else:\r\n",
        "    main_df = main_df.join(df)\r\n",
        "\r\n",
        "  main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\r\n",
        "  main_df.dropna(inplace=True)\r\n",
        "\r\n",
        "# Now lets check the future price of litecoin  of all coins\r\n",
        "\r\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT) \r\n",
        "print(main_df.head())\r\n",
        "\r\n",
        "\r\n",
        "#future price of litecoin(LTC-USD) \r\n",
        "# the 1st column is the \"current\" and the 2nd is the \"future\" after 3 periods \r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\"]].head()) \r\n",
        "\r\n",
        "\r\n",
        "# The map part is what allows us to do this row-by-row for these columns, but also do it quite fast. \r\n",
        "# The list part converts the end result to a list, which we can just set as a column.\r\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\",\"target\" ]].head(12)) \r\n",
        "\r\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            BTC-USD_close  BTC-USD_volume  ...  ETH-USD_volume     future\n",
            "time                                       ...                           \n",
            "1528968720    6487.379883        7.706374  ...       26.019083  96.389999\n",
            "1528968780    6479.410156        3.088252  ...        8.449400  96.519997\n",
            "1528968840    6479.410156        1.404100  ...       26.994646  96.440002\n",
            "1528968900    6479.979980        0.753000  ...       77.355759  96.470001\n",
            "1528968960    6480.000000        1.490900  ...        7.503300  96.400002\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "            LTC-USD_close     future\n",
            "time                                \n",
            "1528968720      96.660004  96.389999\n",
            "1528968780      96.570000  96.519997\n",
            "1528968840      96.500000  96.440002\n",
            "1528968900      96.389999  96.470001\n",
            "1528968960      96.519997  96.400002\n",
            "            LTC-USD_close     future  target\n",
            "time                                        \n",
            "1528968720      96.660004  96.389999       0\n",
            "1528968780      96.570000  96.519997       0\n",
            "1528968840      96.500000  96.440002       0\n",
            "1528968900      96.389999  96.470001       1\n",
            "1528968960      96.519997  96.400002       0\n",
            "1528969020      96.440002  96.400002       0\n",
            "1528969080      96.470001  96.400002       0\n",
            "1528969140      96.400002  96.400002       0\n",
            "1528969200      96.400002  96.400002       0\n",
            "1528969260      96.400002  96.449997       1\n",
            "1528969320      96.400002  96.419998       1\n",
            "1528969380      96.400002  96.400002       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdIbgJ1K6GZz"
      },
      "source": [
        "# 2nd Normalizing and creating sequences for our cryptocurrency predicting RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fecq2wQt9o9c"
      },
      "source": [
        "The first thing I would like to do is separate out our validation/out of sample data. In the past, all we did was shuffle data, then slice it. Does that make sense here though?\r\n",
        "\r\n",
        "The problem with that method is, the data is inherently sequential, so taking sequences that don't come in the future is likely a mistake. This is because sequences in our case, for example, 1 minute apart, will be almost identical. Chances are, the target is also going to be the same (buy or sell). Because of this, any overfitting is likely to actually pour over into the validation set. Instead, we want to slice our validation while it's still in order. I'd like to take the last 5% of the data. To do that, we'll do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cVEugpjMOKF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "192f103e-0670-4e65-f798-b0802ec33529"
      },
      "source": [
        "times = sorted(main_df.index.values) #.index->reference to index, .values->converts to numpy array\r\n",
        "last_5pct = times[-int(0.05*len(times))] # # get the last 5% of the times\r\n",
        "print(last_5pct)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1534922100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_64z_9v-5om"
      },
      "source": [
        "so we have a timestamp (last_5pct=1534922100) so we know that after that time stamp we have the last 5% of our data\r\n",
        "\r\n",
        "### Now we are going to separate our validation data(or out of sample data) and our training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdLjvCPv_Ou8"
      },
      "source": [
        "validation_main_df = main_df[(main_df.index>=last_5pct)] # make the validation data where the index is in the last 5%\r\n",
        "main_df = main_df[(main_df.index<last_5pct)]             # now the main_df is all the data up to the last 5%"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-RJPE1yAwJm"
      },
      "source": [
        "Next, we need to balance and normalize this data. By balance, we want to make sure the classes have equal amounts when training, so our model doesn't just always predict one class.\r\n",
        "\r\n",
        "One way to counteract this is to use class weights, which allows you to weight loss higher for lesser-frequent classifications. That said, I've never personally seen this really be comparable to a real balanced dataset.\r\n",
        "\r\n",
        "We also need to take our data and make sequences from it.\r\n",
        "\r\n",
        "So...we've got some work to do! We'll start by making a function that will process the dataframes, so we can just do something like:\r\n",
        "\r\n",
        "train_x, train_y = preprocess_df(main_df) \r\n",
        "\r\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)\r\n",
        "\r\n",
        "Let's start by removing the future column (the actual target is called literally target and only needed the future column temporarily to create it). Then, we need to scale our data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jop2C611AUiC"
      },
      "source": [
        "from sklearn import preprocessing\r\n",
        "from collections import deque\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_df(df):\r\n",
        "  df = df.drop('future', 1)                          # don't need this anymore.\r\n",
        "\r\n",
        "  for col in df.columns:\r\n",
        "    if col != \"target\":                              # normalize all ... except for the target itself!(its already done)\r\n",
        "      df[col] = df[col].pct_change()                 # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\r\n",
        "      df.dropna(inplace=True)                        #remove the nas created by pct_change\r\n",
        "      df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1\r\n",
        "  \r\n",
        "  df.dropna(inplace=True)                            # cleanup again. Those nasty NaNs love to creep in.\r\n",
        "\r\n",
        "    # Alright, we've normalized and scaled the data! Next up, we need to create our actual sequences. To do this:\r\n",
        "  sequential_data = []                               # this is a list that will CONTAIN the sequences\r\n",
        "  prev_days = deque(maxlen=SEQ_LEN)                  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in.\r\n",
        " \r\n",
        "  for i in df.values:                                #iterate over the values(df.values->converts my dataframe to a list of lists, so it wont contain \"time\" anymore, but its in the order of the index, BUT it is going to contain \"target\" so we have to be careful), i is the row of all the columns(BTC-USD_close,BTC-USD_volume,LTC-USD_close...)\r\n",
        "    prev_days.append([n for n in i[:-1]])            # store all but the target\r\n",
        "    if len(prev_days) == SEQ_LEN:                    # make sure we have 60 sequences!\r\n",
        "      sequential_data.append([np.array(prev_days), i[-1]])   #we are appending out x's and y's(features(=[np.array(prev_days)) and labels(=i[-1]))\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                    # shuffle for good measure.\r\n",
        "\r\n",
        "  # run the 3 below lines to have a better understanding of what is going on\r\n",
        "  # print(df.head())\r\n",
        "  # for c in df.columns:\r\n",
        "  #   print(c)\r\n",
        "  \r\n",
        "\r\n",
        "\r\n",
        "preprocess_df(main_df)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8JTqljFLrT9"
      },
      "source": [
        "We've got our data, we've got sequences, we've got the data normalized, and we've got it scaled. The whole code so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDKPLIDsD1S4",
        "outputId": "9645c305-a38b-4dc5-943d-c45c6a716f3b"
      },
      "source": [
        "import pandas as pd \r\n",
        "import os\r\n",
        "from sklearn import preprocessing\r\n",
        "from collections import deque\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "SEQ_LEN = 60\r\n",
        "FUTURE_PERIOD_PREDICT = 3\r\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\r\n",
        "\r\n",
        "\r\n",
        "def classify(current, future):\r\n",
        "  if float(future)>float(current):\r\n",
        "    return 1                        #BUY\r\n",
        "  else:\r\n",
        "    return 0                        #DONT BUY\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_df(df):\r\n",
        "  df = df.drop('future', 1)                          # don't need this anymore.\r\n",
        "\r\n",
        "  for col in df.columns:\r\n",
        "    if col != \"target\":                              # normalize all ... except for the target itself!(its already done)\r\n",
        "      df[col] = df[col].pct_change()                 # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\r\n",
        "      df.dropna(inplace=True)                        #remove the nas created by pct_change\r\n",
        "      df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1\r\n",
        "  \r\n",
        "  df.dropna(inplace=True)                            # cleanup again. Those nasty NaNs love to creep in.\r\n",
        "\r\n",
        "    # Alright, we've normalized and scaled the data! Next up, we need to create our actual sequences. To do this:\r\n",
        "  sequential_data = []                               # this is a list that will CONTAIN the sequences\r\n",
        "  prev_days = deque(maxlen=SEQ_LEN)                  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in.\r\n",
        " \r\n",
        " # run the 3 below lines to have a better understanding of what is going on\r\n",
        "  # print(df.head())\r\n",
        "  # for c in df.columns:\r\n",
        "  #   print(c)\r\n",
        "\r\n",
        "  for i in df.values:                                #iterate over the values(df.values->converts my dataframe to a list of lists, so it wont contain \"time\" anymore, but its in the order of the index, BUT it is going to contain \"target\" so we have to be careful), i is the row of all the columns(BTC-USD_close,BTC-USD_volume,LTC-USD_close...)\r\n",
        "    prev_days.append([n for n in i[:-1]])            # store all but the target\r\n",
        "    if len(prev_days) == SEQ_LEN:                    # make sure we have 60 sequences!\r\n",
        "      sequential_data.append([np.array(prev_days), i[-1]])   #we are appending out x's and y's(features(=[np.array(prev_days)) and labels(=i[-1]))\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                    # shuffle for good measure.\r\n",
        "\r\n",
        "\r\n",
        "main_df = pd.DataFrame() # begin empty\r\n",
        "\r\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\r\n",
        "for ratio in ratios:\r\n",
        "  # print(ratio)\r\n",
        "  dataset = f\"/content/{ratio}.csv\"\r\n",
        "  # print(dataset)\r\n",
        "\r\n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\r\n",
        "  # print(df.head()) we want to work with close and volume\r\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\r\n",
        "\r\n",
        "  df.set_index(\"time\", inplace=True)\r\n",
        "  df = df[[f'{ratio}_close',f\"{ratio}_volume\"]]\r\n",
        "  # print(df.head())\r\n",
        "\r\n",
        "  # now we want to merge those 4\r\n",
        "  if len(main_df) == 0:           #i.e. is empty\r\n",
        "    main_df = df\r\n",
        "  else:\r\n",
        "    main_df = main_df.join(df)\r\n",
        "\r\n",
        "  main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\r\n",
        "  main_df.dropna(inplace=True)\r\n",
        "\r\n",
        "# Now lets check the future price of litecoin  of all coins\r\n",
        "\r\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT) \r\n",
        "print(main_df.head())\r\n",
        "\r\n",
        "\r\n",
        "#future price of litecoin(LTC-USD) \r\n",
        "# the 1st column is the \"current\" and the 2nd is the \"future\" after 3 periods \r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\"]].head()) \r\n",
        "\r\n",
        "\r\n",
        "# The map part is what allows us to do this row-by-row for these columns, but also do it quite fast. \r\n",
        "# The list part converts the end result to a list, which we can just set as a column.\r\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\",\"target\" ]].head(12)) \r\n",
        "\r\n",
        "preprocess_df(main_df)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            BTC-USD_close  BTC-USD_volume  ...  ETH-USD_volume     future\n",
            "time                                       ...                           \n",
            "1528968720    6487.379883        7.706374  ...       26.019083  96.389999\n",
            "1528968780    6479.410156        3.088252  ...        8.449400  96.519997\n",
            "1528968840    6479.410156        1.404100  ...       26.994646  96.440002\n",
            "1528968900    6479.979980        0.753000  ...       77.355759  96.470001\n",
            "1528968960    6480.000000        1.490900  ...        7.503300  96.400002\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "            LTC-USD_close     future\n",
            "time                                \n",
            "1528968720      96.660004  96.389999\n",
            "1528968780      96.570000  96.519997\n",
            "1528968840      96.500000  96.440002\n",
            "1528968900      96.389999  96.470001\n",
            "1528968960      96.519997  96.400002\n",
            "            LTC-USD_close     future  target\n",
            "time                                        \n",
            "1528968720      96.660004  96.389999       0\n",
            "1528968780      96.570000  96.519997       0\n",
            "1528968840      96.500000  96.440002       0\n",
            "1528968900      96.389999  96.470001       1\n",
            "1528968960      96.519997  96.400002       0\n",
            "1528969020      96.440002  96.400002       0\n",
            "1528969080      96.470001  96.400002       0\n",
            "1528969140      96.400002  96.400002       0\n",
            "1528969200      96.400002  96.400002       0\n",
            "1528969260      96.400002  96.449997       1\n",
            "1528969320      96.400002  96.419998       1\n",
            "1528969380      96.400002  96.400002       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOPpQalWSpOt"
      },
      "source": [
        "# Continuing along in our preprocess_df function, we can balance by doing:\r\n",
        "\r\n",
        "\r\n",
        "we need to have as many buys as we have sells, so we want an approximately 50/50 split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kru_86lSS6un"
      },
      "source": [
        "from sklearn import preprocessing\r\n",
        "from collections import deque\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_df(df):\r\n",
        "  df = df.drop('future', 1)                          # don't need this anymore.\r\n",
        "\r\n",
        "  for col in df.columns:\r\n",
        "    if col != \"target\":                              # normalize all ... except for the target itself!(its already done)\r\n",
        "      df[col] = df[col].pct_change()                 # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\r\n",
        "      df.dropna(inplace=True)                        #remove the nas created by pct_change\r\n",
        "      df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1\r\n",
        "  \r\n",
        "  df.dropna(inplace=True)                            # cleanup again. Those nasty NaNs love to creep in.\r\n",
        "\r\n",
        "    # Alright, we've normalized and scaled the data! Next up, we need to create our actual sequences. To do this:\r\n",
        "  sequential_data = []                               # this is a list that will CONTAIN the sequences\r\n",
        "  prev_days = deque(maxlen=SEQ_LEN)                  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in.\r\n",
        " \r\n",
        "  for i in df.values:                                #iterate over the values(df.values->converts my dataframe to a list of lists, so it wont contain \"time\" anymore, but its in the order of the index, BUT it is going to contain \"target\" so we have to be careful), i is the row of all the columns(BTC-USD_close,BTC-USD_volume,LTC-USD_close...)\r\n",
        "    prev_days.append([n for n in i[:-1]])            # store all but the target\r\n",
        "    if len(prev_days) == SEQ_LEN:                    # make sure we have 60 sequences!\r\n",
        "      sequential_data.append([np.array(prev_days), i[-1]])   #we are appending out x's and y's(features(=[np.array(prev_days)) and labels(=i[-1]))\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                    # shuffle for good measure.\r\n",
        "\r\n",
        "  buys = []                                           # list that will store our buy sequences and targets\r\n",
        "  sells = []                                          # list that will store our sell sequences and targets\r\n",
        "\r\n",
        "  for seq, target in sequential_data:\r\n",
        "    if target == 0:\r\n",
        "      sells.append([seq, target])\r\n",
        "    elif target == 1:\r\n",
        "      buys.append([seq, target])\r\n",
        "\r\n",
        "  random.shuffle(buys)\r\n",
        "  random.shuffle(sells)                               #just to be safe \r\n",
        "\r\n",
        "  lower = min(len(buys), len(sells))                  # what's the shorter length?\r\n",
        "\r\n",
        "  buys = buys[:lower]                                 # make sure both lists are only up to the shortest length.\r\n",
        "  sells = sells[:lower]\r\n",
        "\r\n",
        "  sequential_data = buys + sells\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                     # another shuffle, so the model doesn't get confused with all 1 class then the other.\r\n",
        "\r\n",
        "  # Now, all we need to do is split the data back to featuresets and labels/targets. Easy enough:\r\n",
        "\r\n",
        "  X = []\r\n",
        "  y = []\r\n",
        "\r\n",
        "  for seq, target in sequential_data:\r\n",
        "    X.append(seq)                                     # X is the sequences\r\n",
        "    y.append(target)                                  # y is the targets/labels (buys vs sell/notbuy)\r\n",
        "\r\n",
        "  return np.array(X), y                               \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFey1E15YjY6"
      },
      "source": [
        "## SO... again full code up to this point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5s1WLo5yV-3T",
        "outputId": "e99b0d5f-96a0-4bff-b1e3-66ed9b6c9f7c"
      },
      "source": [
        "import pandas as pd \r\n",
        "import os\r\n",
        "from sklearn import preprocessing\r\n",
        "from collections import deque\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "SEQ_LEN = 60\r\n",
        "FUTURE_PERIOD_PREDICT = 3\r\n",
        "RATIO_TO_PREDICT = \"LTC-USD\"\r\n",
        "\r\n",
        "\r\n",
        "def classify(current, future):\r\n",
        "  if float(future)>float(current):\r\n",
        "    return 1                        #BUY\r\n",
        "  else:\r\n",
        "    return 0                        #DONT BUY\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_df(df):\r\n",
        "  df = df.drop('future', 1)                          # don't need this anymore.\r\n",
        "\r\n",
        "  for col in df.columns:\r\n",
        "    if col != \"target\":                              # normalize all ... except for the target itself!(its already done)\r\n",
        "      df[col] = df[col].pct_change()                 # pct change \"normalizes\" the different currencies (each crypto coin has vastly diff values, we're really more interested in the other coin's movements)\r\n",
        "      df.dropna(inplace=True)                        #remove the nas created by pct_change\r\n",
        "      df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1\r\n",
        "  \r\n",
        "  df.dropna(inplace=True)                            # cleanup again. Those nasty NaNs love to creep in.\r\n",
        "\r\n",
        "    # Alright, we've normalized and scaled the data! Next up, we need to create our actual sequences. To do this:\r\n",
        "  sequential_data = []                               # this is a list that will CONTAIN the sequences\r\n",
        "  prev_days = deque(maxlen=SEQ_LEN)                  # These will be our actual sequences. They are made with deque, which keeps the maximum length by popping out older values as new ones come in.\r\n",
        " \r\n",
        " # run the 3 below lines to have a better understanding of what is going on\r\n",
        "  # print(df.head())\r\n",
        "  # for c in df.columns:\r\n",
        "  #   print(c)\r\n",
        "\r\n",
        "  for i in df.values:                                #iterate over the values(df.values->converts my dataframe to a list of lists, so it wont contain \"time\" anymore, but its in the order of the index, BUT it is going to contain \"target\" so we have to be careful), i is the row of all the columns(BTC-USD_close,BTC-USD_volume,LTC-USD_close...)\r\n",
        "    prev_days.append([n for n in i[:-1]])            # store all but the target\r\n",
        "    if len(prev_days) == SEQ_LEN:                    # make sure we have 60 sequences!\r\n",
        "      sequential_data.append([np.array(prev_days), i[-1]])   #we are appending out x's and y's(features(=[np.array(prev_days)) and labels(=i[-1]))\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                    # shuffle for good measure.\r\n",
        "\r\n",
        "\r\n",
        "  buys = []                                           # list that will store our buy sequences and targets\r\n",
        "  sells = []                                          # list that will store our sell sequences and targets\r\n",
        "\r\n",
        "  for seq, target in sequential_data:\r\n",
        "    if target == 0:\r\n",
        "      sells.append([seq, target])\r\n",
        "    elif target == 1:\r\n",
        "      buys.append([seq, target])\r\n",
        "\r\n",
        "  random.shuffle(buys)\r\n",
        "  random.shuffle(sells)                               #just to be safe \r\n",
        "\r\n",
        "  lower = min(len(buys), len(sells))                  # what's the shorter length?\r\n",
        "\r\n",
        "  buys = buys[:lower]                                 # make sure both lists are only up to the shortest length.\r\n",
        "  sells = sells[:lower]\r\n",
        "\r\n",
        "  sequential_data = buys + sells\r\n",
        "\r\n",
        "  random.shuffle(sequential_data)                     # another shuffle, so the model doesn't get confused with all 1 class then the other.\r\n",
        "\r\n",
        "  # Now, all we need to do is split the data back to featuresets and labels/targets. Easy enough:\r\n",
        "\r\n",
        "  X = []\r\n",
        "  y = []\r\n",
        "\r\n",
        "  for seq, target in sequential_data:\r\n",
        "    X.append(seq)                                     # X is the sequences\r\n",
        "    y.append(target)                                  # y is the targets/labels (buys vs sell/notbuy)\r\n",
        "\r\n",
        "  return np.array(X), y                               # return X and y...and make X a numpy array!\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "main_df = pd.DataFrame() # begin empty\r\n",
        "\r\n",
        "ratios = [\"BTC-USD\", \"LTC-USD\", \"BCH-USD\", \"ETH-USD\"]  # the 4 ratios we want to consider\r\n",
        "for ratio in ratios:\r\n",
        "  # print(ratio)\r\n",
        "  dataset = f\"/content/{ratio}.csv\"\r\n",
        "  # print(dataset)\r\n",
        "\r\n",
        "  df = pd.read_csv(dataset, names=[\"time\", \"low\", \"high\", \"open\", \"close\", \"volume\"])\r\n",
        "  # print(df.head()) we want to work with close and volume\r\n",
        "  df.rename(columns={\"close\": f\"{ratio}_close\", \"volume\": f\"{ratio}_volume\"}, inplace=True)\r\n",
        "\r\n",
        "  df.set_index(\"time\", inplace=True)\r\n",
        "  df = df[[f'{ratio}_close',f\"{ratio}_volume\"]]\r\n",
        "  # print(df.head())\r\n",
        "\r\n",
        "  # now we want to merge those 4\r\n",
        "  if len(main_df) == 0:           #i.e. is empty\r\n",
        "    main_df = df\r\n",
        "  else:\r\n",
        "    main_df = main_df.join(df)\r\n",
        "\r\n",
        "  main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\r\n",
        "  main_df.dropna(inplace=True)\r\n",
        "\r\n",
        "# Now lets check the future price of litecoin  of all coins\r\n",
        "\r\n",
        "main_df['future'] = main_df[f'{RATIO_TO_PREDICT}_close'].shift(-FUTURE_PERIOD_PREDICT) \r\n",
        "print(main_df.head())\r\n",
        "\r\n",
        "\r\n",
        "#future price of litecoin(LTC-USD) \r\n",
        "# the 1st column is the \"current\" and the 2nd is the \"future\" after 3 periods \r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\"]].head()) \r\n",
        "\r\n",
        "\r\n",
        "# The map part is what allows us to do this row-by-row for these columns, but also do it quite fast. \r\n",
        "# The list part converts the end result to a list, which we can just set as a column.\r\n",
        "main_df['target'] = list(map(classify, main_df[f'{RATIO_TO_PREDICT}_close'], main_df['future']))\r\n",
        "print(main_df[[f'{RATIO_TO_PREDICT}_close', \"future\",\"target\" ]].head(12)) \r\n",
        "\r\n",
        "# preprocess_df(main_df)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "            BTC-USD_close  BTC-USD_volume  ...  ETH-USD_volume     future\n",
            "time                                       ...                           \n",
            "1528968720    6487.379883        7.706374  ...       26.019083  96.389999\n",
            "1528968780    6479.410156        3.088252  ...        8.449400  96.519997\n",
            "1528968840    6479.410156        1.404100  ...       26.994646  96.440002\n",
            "1528968900    6479.979980        0.753000  ...       77.355759  96.470001\n",
            "1528968960    6480.000000        1.490900  ...        7.503300  96.400002\n",
            "\n",
            "[5 rows x 9 columns]\n",
            "            LTC-USD_close     future\n",
            "time                                \n",
            "1528968720      96.660004  96.389999\n",
            "1528968780      96.570000  96.519997\n",
            "1528968840      96.500000  96.440002\n",
            "1528968900      96.389999  96.470001\n",
            "1528968960      96.519997  96.400002\n",
            "            LTC-USD_close     future  target\n",
            "time                                        \n",
            "1528968720      96.660004  96.389999       0\n",
            "1528968780      96.570000  96.519997       0\n",
            "1528968840      96.500000  96.440002       0\n",
            "1528968900      96.389999  96.470001       1\n",
            "1528968960      96.519997  96.400002       0\n",
            "1528969020      96.440002  96.400002       0\n",
            "1528969080      96.470001  96.400002       0\n",
            "1528969140      96.400002  96.400002       0\n",
            "1528969200      96.400002  96.400002       0\n",
            "1528969260      96.400002  96.449997       1\n",
            "1528969320      96.400002  96.419998       1\n",
            "1528969380      96.400002  96.400002       0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqGwxP_VZvzF"
      },
      "source": [
        "We can now preprocess our data with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_uxK3mmZwmt"
      },
      "source": [
        "train_x, train_y = preprocess_df(main_df)\r\n",
        "validation_x, validation_y = preprocess_df(validation_main_df)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWZkgeTOaxN9"
      },
      "source": [
        "Okay, let's print some stats real quick to make sure things are what we expect:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUT2lHwzauLU",
        "outputId": "385ba6e1-9804-417a-d556-73c4ee2e53b7"
      },
      "source": [
        "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\r\n",
        "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\r\n",
        "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train data: 81812 validation: 3860\n",
            "Dont buys: 40906, buys: 40906\n",
            "VALIDATION Dont buys: 1930, buys: 1930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTbIXunVa4Mg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}